\section{Background on Reinforcement Learning}

\begin{notes}
    (10-15 pages)

    cover:
    \begin{itemize}
        \item MDP
        \item policy
        \item q function
        \item Bellman equations
        \item q learning
    \end{itemize}
\end{notes}

The goal of reinforcement learning is to learn a task by maximizing the total rewards along sequences of decisions called episodes. The difficulty is that decisions can have delayed consequences.

We have an agent that acts in an environment. How the environment behaves is unknown to the agent, i.e. how the environment goes from state to state is a black box, its transition probabilities are not known. Furthermore for each action the agent takes it receives a reward from the environment but how these rewards are determined is also not visible to the agent. So the environment is a complete black box to the agent.

Consider the following example as a demonstration of the reinforcement learning framework. In a grid-based office environment a robot is tasked with bringing coffee from the kitchen to a particular office, see figure \ref{fig:officegrid} for the layout.

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height=5cm]{example-image-a}
    \caption{office world...}
    \label{fig:officegrid}
\end{figure}

The environment starts in state $s_0 \in S$ and for each state $s_t$ at time $t \in \N$ the agent has to decide on an action $a_t \in A$ like going up, down, left, right or use the coffee machine. After executing the action the environment changes to a new state $s_{t+1}$ and rewards the robot with a reward $r_{t+1} \in \R$. This feedback loop is shown as a diagram in figure \ref{fig:rl_framework}.

\usetikzlibrary{graphs, quotes}
\begin{figure}[h]
    \centering
    \tikz{
    \node[draw, rectangle, minimum width=5em, minimum height=3em] (a) at (0,0) {Agent};
    \node[draw, rectangle, minimum width=5em, minimum height=3em] (e) at (6,0) {Environment};

    \graph[multi, grow right sep]{
    (a) ->["action $a_t$", bend left] (e);
    (e) ->["reward $r_{t+1}$", bend left=50] (a);
    (e) ->["state $s_{t+1}$", bend left=30, swap] (a);
    };
    }
    \caption{Diagram of the reinforcement learning feedback loop.}
    \label{fig:rl_framework}
\end{figure}

The usual way to model this is via a Markov decision process. What represents what? what is the idea, why markov process??
describe probability measure $\P$ and stochastic processes to more rigorously define the quantities and then give the implicit definition using (S, A, P, R, gamma)

\begin{definition}
    \label{def:mdp}
    A \emph{Markov Decision Process (MDP)} is given by a tuple $(S, A, P, R, \gamma)$, where $S$ is a finite set of \emph{states}, $A$ is a finite set of \emph{actions}, $P: S \times A \times S \to [0,1]$ is the \emph{transition probability distribution}, $R: S \times A \times S \to \R$ is the \emph{reward function} and $\gamma \in [0,1]$ is a \emph{discount factor}.
\end{definition}

This MDP models the diagram in figure \ref{fig:rl_framework} from above as follows: From a given state $s \in S$ the agent chooses an action $a \in A$ and the environment changes to state $s' \in S$ with probability $P_a(s, s')$ and gives reward $R_a(s, s')$. The discount factor will become relevant in a moment but in essence a lower discount factor would motivate the agent to take actions based on the reward sooner rather than later as with a higher discount factor.

In the office world example ...

\begin{definition}
    \label{def:policy}
    A policy $\pi: S \times A \to [0,1]$ is a probabiliy distribution that prescribes an action $a \in A$ to be taken for a given state $s \in S$ with probability $\pi(a | s)$.
\end{definition}

In the office world example a policy could be to go left when in room 1 and go right when in room 2 or stand still when not in either room. Of course this would not be a good policy, but what constitutes "good" will be defined with the Q-function.

\begin{definition}
    \label{def:q}
    The Q-function $q_\pi: S \times A \to \R$ under a policy $\pi$ is

\end{definition}

prove existance of optimal policy

prove Bellman optimality equation for MDP (write about the dynamical programming approach % https://www.wikiwand.com/en/Dynamic_programming)